{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from collections import defaultdict, namedtuple\n",
    "from io import open\n",
    "import math\n",
    "import os\n",
    "from random import shuffle, uniform\n",
    "\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Input, Model, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    \"\"\"\n",
    "    This method loads and returns the data in filename. If the data is labelled training data, it returns labels too.\n",
    "\n",
    "    Parameters:\n",
    "        filename: the location of the training or test data you want to load.\n",
    "\n",
    "    Returns:\n",
    "        data: a list of InstanceData objects from that data type and track.\n",
    "        labels (optional): if you specified training data, a dict of instance_id:label pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    # 'data' stores a list of 'InstanceData's as values.\n",
    "    data = []\n",
    "\n",
    "    # If this is training data, then 'labels' is a dict that contains instance_ids as keys and labels as values.\n",
    "    training = False\n",
    "    if filename.find('train') != -1:\n",
    "        training = True\n",
    "\n",
    "    if training:\n",
    "        labels = dict()\n",
    "\n",
    "    num_exercises = 0\n",
    "    print('Loading instances...')\n",
    "    instance_properties = dict()\n",
    "\n",
    "    with open(filename, 'rt') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "\n",
    "            # If there's nothing in the line, then we're done with the exercise. Print if needed, otherwise continue\n",
    "            if len(line) == 0:\n",
    "                num_exercises += 1\n",
    "                if num_exercises % 100000 == 0:\n",
    "                    print('Loaded ' + str(len(data)) + ' instances across ' + str(num_exercises) + ' exercises...')\n",
    "                instance_properties = dict()\n",
    "\n",
    "            # If the line starts with #, then we're beginning a new exercise\n",
    "            elif line[0] == '#':\n",
    "                if 'prompt' in line:\n",
    "                    instance_properties['prompt'] = line.split(':')[1]\n",
    "                else:\n",
    "                    list_of_exercise_parameters = line[2:].split()\n",
    "                    for exercise_parameter in list_of_exercise_parameters:\n",
    "                        [key, value] = exercise_parameter.split(':')\n",
    "                        if key == 'countries':\n",
    "                            value = value.split('|')\n",
    "                        elif key == 'days':\n",
    "                            value = float(value)\n",
    "                        elif key == 'time':\n",
    "                            if value == 'null':\n",
    "                                value = None\n",
    "                            else:\n",
    "                                assert '.' not in value\n",
    "                                value = int(value)\n",
    "                        instance_properties[key] = value\n",
    "\n",
    "            # Otherwise we're parsing a new Instance for the current exercise\n",
    "            else:\n",
    "                line = line.split()\n",
    "                if training:\n",
    "                    assert len(line) == 7\n",
    "                else:\n",
    "                    assert len(line) == 6\n",
    "                assert len(line[0]) == 12\n",
    "\n",
    "                instance_properties['instance_id'] = line[0]\n",
    "                instance_properties['token'] = line[1]\n",
    "                instance_properties['part_of_speech'] = line[2]\n",
    "\n",
    "                # instance_properties['morphological_features'] = dict()\n",
    "                # for l in line[3].split('|'):\n",
    "                #     [key, value] = l.split('=')\n",
    "                #     if key == 'Person':\n",
    "                #         value = int(value)\n",
    "                #     instance_properties['morphological_features'][key] = value\n",
    "\n",
    "                instance_properties['dependency_label'] = line[4]\n",
    "                instance_properties['dependency_edge_head'] = int(line[5])\n",
    "                if training:\n",
    "                    # label = float(line[6])\n",
    "                    # labels[instance_properties['instance_id']] = label\n",
    "                    instance_properties['label'] = float(line[6])\n",
    "                # data.append(InstanceData(instance_properties=instance_properties))\n",
    "                data.append(deepcopy(instance_properties))\n",
    "\n",
    "        print('Done loading ' + str(len(data)) + ' instances across ' + str(num_exercises) +\n",
    "              ' exercises.\\n')\n",
    "\n",
    "    return data\n",
    "\n",
    "# class InstanceData(object):\n",
    "#     \"\"\"\n",
    "#     A bare-bones class to store the included properties of each instance. This is meant to act as easy access to the\n",
    "#     data, and provides a launching point for deriving your own features from the data.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, instance_properties):\n",
    "\n",
    "#         # Parameters specific to this instance\n",
    "#         self.instance_id = instance_properties['instance_id']\n",
    "#         self.token = instance_properties['token']\n",
    "#         self.part_of_speech = instance_properties['part_of_speech']\n",
    "#         self.morphological_features = instance_properties['morphological_features']\n",
    "#         self.dependency_label = instance_properties['dependency_label']\n",
    "#         self.dependency_edge_head = instance_properties['dependency_edge_head']\n",
    "\n",
    "#         # Derived parameters specific to this instance\n",
    "#         self.exercise_index = int(self.instance_id[8:10])\n",
    "#         self.token_index = int(self.instance_id[10:12])\n",
    "\n",
    "#         # Derived parameters specific to this exercise\n",
    "#         self.exercise_id = self.instance_id[:10]\n",
    "\n",
    "#         # Parameters shared across the whole session\n",
    "#         self.user = instance_properties['user']\n",
    "#         self.countries = instance_properties['countries']\n",
    "#         self.days = instance_properties['days']\n",
    "#         self.client = instance_properties['client']\n",
    "#         self.session = instance_properties['session']\n",
    "#         self.format = instance_properties['format']\n",
    "#         self.time = instance_properties['time']\n",
    "#         self.prompt = instance_properties.get('prompt', None)\n",
    "\n",
    "#         # Derived parameters shared across the whole session\n",
    "#         self.session_id = self.instance_id[:8]\n",
    "\n",
    "#     def to_features(self):\n",
    "#         \"\"\"\n",
    "#         Prepares those features that we wish to use in the LogisticRegression example in this file. We introduce a bias,\n",
    "#         and take a few included features to use. Note that this dict restructures the corresponding features of the\n",
    "#         input dictionary, 'instance_properties'.\n",
    "\n",
    "#         Returns:\n",
    "#             to_return: a representation of the features we'll use for logistic regression in a dict. A key/feature is a\n",
    "#                 key/value pair of the original 'instance_properties' dict, and we encode this feature as 1.0 for 'hot'.\n",
    "#         \"\"\"\n",
    "#         to_return = dict()\n",
    "\n",
    "#         to_return['bias'] = 1.0\n",
    "#         to_return['user:' + self.user] = 1.0\n",
    "#         to_return['format:' + self.format] = 1.0\n",
    "#         to_return['token:' + self.token.lower()] = 1.0\n",
    "\n",
    "#         to_return['part_of_speech:' + self.part_of_speech] = 1.0\n",
    "#         # for morphological_feature in self.morphological_features:\n",
    "#         #     to_return['morphological_feature:' + morphological_feature] = 1.0\n",
    "#         to_return['dependency_label:' + self.dependency_label] = 1.0\n",
    "\n",
    "#         return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def building_dataset(train, test):\n",
    "    training_data = load_data(train)\n",
    "    test_data = load_data(test)\n",
    "\n",
    "    return training_data, test_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading instances...\n",
      "Loaded 317049 instances across 100000 exercises...\n",
      "Loaded 635368 instances across 200000 exercises...\n",
      "Loaded 951536 instances across 300000 exercises...\n",
      "Loaded 1271940 instances across 400000 exercises...\n",
      "Loaded 1591344 instances across 500000 exercises...\n",
      "Loaded 1911212 instances across 600000 exercises...\n",
      "Loaded 2227444 instances across 700000 exercises...\n",
      "Loaded 2546704 instances across 800000 exercises...\n",
      "Done loading 2622957 instances across 824012 exercises.\n",
      "\n",
      "Loading instances...\n",
      "Loaded 337728 instances across 100000 exercises...\n",
      "Done loading 386604 instances across 114586 exercises.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data, test_data = building_dataset(\"data_en_es/en_es.slam.20190204.train\", \"data_en_es/en_es.slam.20190204.test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_key(test_data, test_key):\n",
    "    for j in range(len(test_data)):\n",
    "        test_data[j]['label'] = None\n",
    "\n",
    "    print('Loading labels...')\n",
    "    count = 0\n",
    "    with open(test_key, 'rt', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            instance_properties = {}\n",
    "            instance_properties['instance_id'], instance_properties['label'] = line.split()\n",
    "            instance_properties['label'] = float(instance_properties['label'])\n",
    "            for instance_data in test_data:\n",
    "                if instance_properties['instance_id'] in instance_data['instance_id']:\n",
    "                    instance_data['label'] = instance_properties['label']\n",
    "            count += 1\n",
    "            if count % 10000 == 0:\n",
    "                print(\"{} key labels proccessed\".format(count))\n",
    "           \n",
    "\n",
    "    print(test_data)          \n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading labels...\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-a17f200a31c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0madd_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_en_es/en_es.slam.20190204.test.key\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-65-d5dac6e1b97b>\u001b[0m in \u001b[0;36madd_key\u001b[0;34m(test_data, test_key)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0minstance_properties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance_properties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0minstance_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0minstance_properties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'instance_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minstance_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'instance_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                     \u001b[0minstance_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance_properties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "add_key(test_data, \"data_en_es/en_es.slam.20190204.test.key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_pd = pd.DataFrame.from_dict(training_data)\n",
    "test_data_pd = pd.DataFrame.from_dict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "              prompt      user countries   days client session  \\\n",
       "0    Yo soy un niño.  XEinXf5+      [CO]  0.003    web  lesson   \n",
       "1    Yo soy un niño.  XEinXf5+      [CO]  0.003    web  lesson   \n",
       "2    Yo soy un niño.  XEinXf5+      [CO]  0.003    web  lesson   \n",
       "3    Yo soy un niño.  XEinXf5+      [CO]  0.003    web  lesson   \n",
       "4  Yo soy de México.  XEinXf5+      [CO]  0.005    web  lesson   \n",
       "\n",
       "              format  time   instance_id token part_of_speech  \\\n",
       "0  reverse_translate   9.0  DRihrVmh0101     I           PRON   \n",
       "1  reverse_translate   9.0  DRihrVmh0102    am           VERB   \n",
       "2  reverse_translate   9.0  DRihrVmh0103     a            DET   \n",
       "3  reverse_translate   9.0  DRihrVmh0104   boy           NOUN   \n",
       "4  reverse_translate  12.0  TOeLHxLS0101     I           PRON   \n",
       "\n",
       "  dependency_label  dependency_edge_head  label  \n",
       "0            nsubj                     4    0.0  \n",
       "1              cop                     4    0.0  \n",
       "2              det                     4    0.0  \n",
       "3             ROOT                     0    0.0  \n",
       "4            nsubj                     4    0.0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>user</th>\n      <th>countries</th>\n      <th>days</th>\n      <th>client</th>\n      <th>session</th>\n      <th>format</th>\n      <th>time</th>\n      <th>instance_id</th>\n      <th>token</th>\n      <th>part_of_speech</th>\n      <th>dependency_label</th>\n      <th>dependency_edge_head</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Yo soy un niño.</td>\n      <td>XEinXf5+</td>\n      <td>[CO]</td>\n      <td>0.003</td>\n      <td>web</td>\n      <td>lesson</td>\n      <td>reverse_translate</td>\n      <td>9.0</td>\n      <td>DRihrVmh0101</td>\n      <td>I</td>\n      <td>PRON</td>\n      <td>nsubj</td>\n      <td>4</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Yo soy un niño.</td>\n      <td>XEinXf5+</td>\n      <td>[CO]</td>\n      <td>0.003</td>\n      <td>web</td>\n      <td>lesson</td>\n      <td>reverse_translate</td>\n      <td>9.0</td>\n      <td>DRihrVmh0102</td>\n      <td>am</td>\n      <td>VERB</td>\n      <td>cop</td>\n      <td>4</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Yo soy un niño.</td>\n      <td>XEinXf5+</td>\n      <td>[CO]</td>\n      <td>0.003</td>\n      <td>web</td>\n      <td>lesson</td>\n      <td>reverse_translate</td>\n      <td>9.0</td>\n      <td>DRihrVmh0103</td>\n      <td>a</td>\n      <td>DET</td>\n      <td>det</td>\n      <td>4</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Yo soy un niño.</td>\n      <td>XEinXf5+</td>\n      <td>[CO]</td>\n      <td>0.003</td>\n      <td>web</td>\n      <td>lesson</td>\n      <td>reverse_translate</td>\n      <td>9.0</td>\n      <td>DRihrVmh0104</td>\n      <td>boy</td>\n      <td>NOUN</td>\n      <td>ROOT</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Yo soy de México.</td>\n      <td>XEinXf5+</td>\n      <td>[CO]</td>\n      <td>0.005</td>\n      <td>web</td>\n      <td>lesson</td>\n      <td>reverse_translate</td>\n      <td>12.0</td>\n      <td>TOeLHxLS0101</td>\n      <td>I</td>\n      <td>PRON</td>\n      <td>nsubj</td>\n      <td>4</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "training_data_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data):\n",
    "    cols = ['user', 'countries', 'days', 'session', 'format', 'time','token', 'part_of_speech', 'dependency_label', 'label']\n",
    "\n",
    "    df = pd.DataFrame(data, columns = cols)\n",
    "    features = df.values.tolist()\n",
    "\n",
    "    # # Mapping feature to an index\n",
    "    # feature_dict = dict()\n",
    "    # count = 0\n",
    "\n",
    "    # for instance_data in data:\n",
    "    #     for key in instance_data.to_features().keys():\n",
    "    #        if key not in feature_dict:\n",
    "    #            feature_dict[key] = count\n",
    "    #            count += 1\n",
    "    # feature_len = len(feature_dict.keys())\n",
    "\n",
    "    # return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_features(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(data):\n",
    "    \"\"\"\n",
    "    Mapping of each distinct feature to an unique index in a dictionary is used to create an embedding matrix for \n",
    "    each said distinct feature. Each individual embedding matrix will be concatenated\n",
    "    together to create one large embedding matrix.\n",
    "\n",
    "    Parameters:\n",
    "        data: a list of InstanceData objects from that data type and track.\n",
    "    Return:\n",
    "        feature_maxtrix_concat: concatenated embedding matrix\n",
    "    \"\"\"     \n",
    "    feature_dict = extract_features(data)  \n",
    "\n",
    "    # Creating embedding matrices for each feature\n",
    "    print(\"Building embedding matrix...\")\n",
    "    users, formats, tokens = [], [], []\n",
    "    for key in feature_dict:\n",
    "        if \"user:\" in key:\n",
    "            users.append(feature_dict[key])\n",
    "        if \"format:\" in key:\n",
    "            formats.append(feature_dict[key])\n",
    "        if \"token:\" in key:\n",
    "            tokens.append(feature_dict[key])\n",
    "\n",
    "    # Embedding layers\n",
    "    user_tensor, format_tensor, token_tensor = Input(shape = (None, ), name = \"users\"), Input(shape = (None, ), name = \"formats\"), Input(shape = (None, ), name = \"tokens\")\n",
    "    user_embed = layers.Embedding(len(users), 128) (user_tensor)\n",
    "    format_embed = layers.Embedding(len(formats), 16) (format_tensor)\n",
    "    token_embed = layers.Embedding(len(tokens), 512) (token_tensor)\n",
    "\n",
    "    feature_matrix = [user_embed, format_embed, token_embed]\n",
    "    feature_matrix_concat = layers.Concatenate()(feature_matrix)\n",
    "\n",
    "    print(\"Embedding matrix: {}\".format(feature_matrix_concat))\n",
    "\n",
    "    # Help hopefully make network train faster\n",
    "    feature_matrix_concat = layers.BatchNormalization() (feature_matrix_concat)\n",
    "    input_tensor = [user_tensor, format_tensor, token_tensor]\n",
    "\n",
    "    return feature_matrix_concat, input_tensor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM(train_data, labels = None):\n",
    "   batch_size = 32\n",
    "   epochs = 10 \n",
    "\n",
    "   embedding, input_tensor = create_embeddings(train_data)\n",
    "   \n",
    "   # Generate the model\n",
    "   X = layers.LSTM(units = 256, return_sequences= True) (embedding)\n",
    "   X = layers.Dropout(0.25) (X)\n",
    "   X = layers.Dense(units = 128) (X)\n",
    "   X = layers.Activation('softmax') (X)\n",
    "\n",
    "   LSTM_model = Model(input_tensor, X)\n",
    "   LSTM_model.compile(optimizer = 'Adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "   LSTM_model.summary()\n",
    "\n",
    "\n",
    "   # LSTM_hist = model.fit(\n",
    "   #    x = , \n",
    "   #    y = , \n",
    "   #    batch_size = batch_size,\n",
    "   #    epochs = epochs)\n",
    "\n",
    "   # return LSTM_hist\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Building embedding matrix...\nEmbedding matrix: Tensor(\"concatenate_1/Identity:0\", shape=(None, None, 80), dtype=float32)\nModel: \"model_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nusers (InputLayer)              [(None, None)]       0                                            \n__________________________________________________________________________________________________\nformats (InputLayer)            [(None, None)]       0                                            \n__________________________________________________________________________________________________\ntokens (InputLayer)             [(None, None)]       0                                            \n__________________________________________________________________________________________________\nembedding_3 (Embedding)         (None, None, 32)     82976       users[0][0]                      \n__________________________________________________________________________________________________\nembedding_4 (Embedding)         (None, None, 16)     48          formats[0][0]                    \n__________________________________________________________________________________________________\nembedding_5 (Embedding)         (None, None, 32)     62944       tokens[0][0]                     \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, None, 80)     0           embedding_3[0][0]                \n                                                                 embedding_4[0][0]                \n                                                                 embedding_5[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_1 (BatchNor (None, None, 80)     320         concatenate_1[0][0]              \n__________________________________________________________________________________________________\nlstm_1 (LSTM)                   (None, None, 256)    345088      batch_normalization_1[0][0]      \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, None, 256)    0           lstm_1[0][0]                     \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, None, 128)    32896       dropout_1[0][0]                  \n__________________________________________________________________________________________________\nactivation_1 (Activation)       (None, None, 128)    0           dense_1[0][0]                    \n==================================================================================================\nTotal params: 524,272\nTrainable params: 524,112\nNon-trainable params: 160\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "LSTM(training_data, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}