{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from collections import defaultdict, namedtuple\n",
    "from io import open\n",
    "import math\n",
    "import os\n",
    "from random import shuffle, uniform\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Input, Model, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, key = None):\n",
    "    \"\"\"\n",
    "    This method loads and returns the data in filename. If the data is labelled training data, it returns labels too.\n",
    "\n",
    "    Parameters:\n",
    "        filename: the location of the training or test data you want to load.\n",
    "\n",
    "    Returns:\n",
    "        data: a list of InstanceData objects from that data type and track.\n",
    "        labels (optional): if you specified training data, a dict of instance_id:label pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    # 'data' stores a list of 'InstanceData's as values.\n",
    "    data = []\n",
    "\n",
    "    # If this is training data, then 'labels' is a dict that contains instance_ids as keys and labels as values.\n",
    "    training = False\n",
    "    if filename.find('train') != -1:\n",
    "        training = True\n",
    "\n",
    "    if training:\n",
    "        labels = dict()\n",
    "\n",
    "    test_key = [] \n",
    "    if key:    \n",
    "        print('Loading test labels...')\n",
    "        with open(key, 'rt', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                temp = dict()\n",
    "                temp['instance_id'], temp['label'] = line.split()\n",
    "                temp['label'] = float(temp['label'])\n",
    "                test_key.append(temp['label'])\n",
    "\n",
    "    num_exercises = 0\n",
    "    print('Loading instances...')\n",
    "    instance_properties = dict()\n",
    "\n",
    "    with open(filename, 'rt') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "\n",
    "            # If there's nothing in the line, then we're done with the exercise. Print if needed, otherwise continue\n",
    "            if len(line) == 0:\n",
    "                num_exercises += 1\n",
    "                if num_exercises % 100000 == 0:\n",
    "                    print('Loaded ' + str(len(data)) + ' instances across ' + str(num_exercises) + ' exercises...')\n",
    "                instance_properties = dict()\n",
    "\n",
    "            # If the line starts with #, then we're beginning a new exercise\n",
    "            elif line[0] == '#':\n",
    "                if 'prompt' in line:\n",
    "                    instance_properties['prompt'] = line.split(':')[1]\n",
    "                else:\n",
    "                    list_of_exercise_parameters = line[2:].split()\n",
    "                    for exercise_parameter in list_of_exercise_parameters:\n",
    "                        [key, value] = exercise_parameter.split(':')\n",
    "                        if key == 'countries':\n",
    "                            value = value.split('|')\n",
    "                        elif key == 'days':\n",
    "                            value = float(value)\n",
    "                        elif key == 'time':\n",
    "                            if value == 'null':\n",
    "                                value = None\n",
    "                            else:\n",
    "                                assert '.' not in value\n",
    "                                value = int(value)\n",
    "                        instance_properties[key] = value\n",
    "\n",
    "            # Otherwise we're parsing a new Instance for the current exercise\n",
    "            else:\n",
    "                line = line.split()\n",
    "                if training:\n",
    "                    assert len(line) == 7\n",
    "                else:\n",
    "                    assert len(line) == 6\n",
    "                assert len(line[0]) == 12\n",
    "\n",
    "                instance_properties['instance_id'] = line[0]\n",
    "                instance_properties['token'] = line[1]\n",
    "                instance_properties['part_of_speech'] = line[2]\n",
    "\n",
    "                # instance_properties['morphological_features'] = dict()\n",
    "                # for l in line[3].split('|'):\n",
    "                #     [key, value] = l.split('=')\n",
    "                #     if key == 'Person':\n",
    "                #         value = int(value)\n",
    "                #     instance_properties['morphological_features'][key] = value\n",
    "\n",
    "                instance_properties['dependency_label'] = line[4]\n",
    "                instance_properties['dependency_edge_head'] = int(line[5])\n",
    "                if training:\n",
    "                    label = float(line[6])\n",
    "                    labels[instance_properties['instance_id']] = label\n",
    "                    instance_properties['label'] = float(line[6])\n",
    "                if key and test_key != []:\n",
    "                    instance_properties['label'] = test_key.pop(0)\n",
    "                data.append(InstanceData(instance_properties=instance_properties))\n",
    "\n",
    "        print('Done loading ' + str(len(data)) + ' instances across ' + str(num_exercises) +\n",
    "              ' exercises.\\n')\n",
    "\n",
    "    # if training: return data, labels\n",
    "\n",
    "    return data\n",
    "\n",
    "class InstanceData(object):\n",
    "    \"\"\"\n",
    "    A bare-bones class to store the included properties of each instance. This is meant to act as easy access to the\n",
    "    data, and provides a launching point for deriving your own features from the data.\n",
    "    \"\"\"\n",
    "    def __init__(self, instance_properties):\n",
    "\n",
    "        # Parameters specific to this instance\n",
    "        self.instance_id = instance_properties['instance_id']\n",
    "        self.token = instance_properties['token']\n",
    "        self.part_of_speech = instance_properties['part_of_speech']\n",
    "        # self.morphological_features = instance_properties['morphological_features']\n",
    "        self.dependency_label = instance_properties['dependency_label']\n",
    "        self.dependency_edge_head = instance_properties['dependency_edge_head']\n",
    "\n",
    "        # Derived parameters specific to this instance\n",
    "        self.exercise_index = int(self.instance_id[8:10])\n",
    "        self.token_index = int(self.instance_id[10:12])\n",
    "\n",
    "        # Derived parameters specific to this exercise\n",
    "        self.exercise_id = self.instance_id[:10]\n",
    "\n",
    "        # Parameters shared across the whole session\n",
    "        self.user = instance_properties['user']\n",
    "        self.countries = instance_properties['countries']\n",
    "        self.days = instance_properties['days']\n",
    "        self.client = instance_properties['client']\n",
    "        self.session = instance_properties['session']\n",
    "        self.format = instance_properties['format']\n",
    "        self.time = instance_properties['time']\n",
    "        self.prompt = instance_properties.get('prompt', None)\n",
    "\n",
    "        # Label\n",
    "        self.label = instance_properties['label']\n",
    "\n",
    "        # Derived parameters shared across the whole session\n",
    "        self.session_id = self.instance_id[:8]\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self.label \n",
    "    def get_exercise_id(self):\n",
    "        return self.exercise_id\n",
    "    def get_format(self):\n",
    "        return 'format:' + self.format\n",
    "    def get_user(self):\n",
    "        return 'user:' + self.user\n",
    "    def get_token(self):\n",
    "        return 'token:' + self.token.lower()\n",
    "    def get_part_of_speech(self):\n",
    "        return 'part_of_speech:' + self.part_of_speech\n",
    "\n",
    "    def to_features(self):\n",
    "        \"\"\"\n",
    "        Prepares those features that we wish to use in the LogisticRegression example in this file. We introduce a bias,\n",
    "        and take a few included features to use. Note that this dict restructures the corresponding features of the\n",
    "        input dictionary, 'instance_properties'.\n",
    "\n",
    "        Returns:\n",
    "            to_return: a representation of the features we'll use for logistic regression in a dict. A key/feature is a\n",
    "                key/value pair of the original 'instance_properties' dict, and we encode this feature as 1.0 for 'hot'.\n",
    "        \"\"\"\n",
    "        to_return = dict()\n",
    "\n",
    "        to_return['bias'] = 1.0\n",
    "        to_return['user:' + self.user] = 1.0\n",
    "        to_return['format:' + self.format] = 1.0\n",
    "        to_return['token:' + self.token.lower()] = 1.0\n",
    "\n",
    "        to_return['part_of_speech:' + self.part_of_speech] = 1.0\n",
    "        # for morphological_feature in self.morphological_features:\n",
    "        #     to_return['morphological_feature:' + morphological_feature] = 1.0\n",
    "        to_return['dependency_label:' + self.dependency_label] = 1.0\n",
    "\n",
    "        return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_dataset(train, test, key):\n",
    "    print(\"Getting training data...\")\n",
    "    training_data = load_data(train)\n",
    "\n",
    "    print(\"Getting test data...\")\n",
    "    test_data = load_data(test, key)\n",
    "\n",
    "    return training_data, test_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Getting training data...\n",
      "Loading instances...\n",
      "Loaded 317049 instances across 100000 exercises...\n",
      "Loaded 635368 instances across 200000 exercises...\n",
      "Loaded 951536 instances across 300000 exercises...\n",
      "Loaded 1271940 instances across 400000 exercises...\n",
      "Loaded 1591344 instances across 500000 exercises...\n",
      "Loaded 1911212 instances across 600000 exercises...\n",
      "Loaded 2227444 instances across 700000 exercises...\n",
      "Loaded 2546704 instances across 800000 exercises...\n",
      "Done loading 2622957 instances across 824012 exercises.\n",
      "\n",
      "Getting test data...\n",
      "Loading test labels...\n",
      "Loading instances...\n",
      "Loaded 337728 instances across 100000 exercises...\n",
      "Done loading 386604 instances across 114586 exercises.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data, test_data = get_raw_dataset(\"data_en_es/en_es.slam.20190204.train\", \"data_en_es/en_es.slam.20190204.test\", \"data_en_es/en_es.slam.20190204.test.key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_mapping(data):\n",
    "    users, formats, tokens, part_of_speeches = dict(), dict(), dict(), dict()\n",
    "    u_count, f_count, t_count, pos_count = 0, 0, 0, 0\n",
    "    \n",
    "    for instance_data in data:\n",
    "        for key in instance_data.to_features().keys():\n",
    "            if 'user:' in key and key not in users:\n",
    "                users[key] = u_count\n",
    "                u_count += 1\n",
    "            if 'format:' in key and key not in formats:\n",
    "                formats[key] = f_count\n",
    "                f_count += 1\n",
    "            if 'token:' in key and key not in tokens:\n",
    "                tokens[key] = t_count\n",
    "                t_count += 1\n",
    "            if 'part_of_speech' in key and key not in part_of_speeches:\n",
    "                part_of_speeches[key] = pos_count\n",
    "                pos_count += 1\n",
    "\n",
    "  \n",
    "    \n",
    "    return users, formats, tokens, part_of_speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "users, formats, tokens, part_of_speeches = feature_mapping(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_formatted_dataset(data, max_sequence_length = 8):\n",
    "    users, formats, tokens, part_of_speeches = feature_mapping(data)\n",
    "    \n",
    "    # exercise_word = [[data[0].get_token(), data[0].get_user(), data[0].get_format(), data[0].get_part_of_speech()]]\n",
    "\n",
    "    exercise = [[tokens[data[0].get_token()], users[data[0].get_user()], formats[data[0].get_format()], part_of_speeches[data[0].get_part_of_speech()]]]\n",
    "    prev_exercise_id = data[0].get_exercise_id()\n",
    "    test = None\n",
    "    for i in range(1, len(data)): \n",
    "        if prev_exercise_id == data[i].get_exercise_id():\n",
    "            token_info = [[tokens[data[i].get_token()], users[data[i].get_user()], formats[data[i].get_format()], part_of_speeches[data[i].get_part_of_speech()]]]\n",
    "            exercise = np.append(exercise, token_info, axis = 0)\n",
    "\n",
    "            # token_word = [[data[i].get_token(), data[i].get_user(), data[i].get_format(), data[i].get_part_of_speech()]]\n",
    "            # exercise_word = np.append(exercise_word, token_word, axis = 0)\n",
    "        else:\n",
    "            prev_exercise_id = data[i].get_exercise_id()\n",
    "            token_info = [[tokens[data[i].get_token()], users[data[i].get_user()], formats[data[i].get_format()], part_of_speeches[data[i].get_part_of_speech()]]]\n",
    "            exercise = np.append(exercise, token_info, axis = 0)\n",
    "\n",
    "            # token_word = [[data[i].get_token(), data[i].get_user(), data[i].get_format(), data[i].get_part_of_speech()]]\n",
    "            # exercise_word = np.append(exercise_word, token_word, axis = 0)\n",
    "        if i % 100000 == 0:\n",
    "            print(\"Processed {} of {} instances\".format(i, len(data)))\n",
    "    print(exercise)\n",
    "    # print(exercise_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processed 100000 of 2622957 instances\n",
      "Processed 200000 of 2622957 instances\n",
      "Processed 300000 of 2622957 instances\n",
      "Processed 400000 of 2622957 instances\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-dbad719e7f9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbuild_formatted_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-126-78340101e85f>\u001b[0m in \u001b[0;36mbuild_formatted_dataset\u001b[0;34m(data, max_sequence_length)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mprev_exercise_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_exercise_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mtoken_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpart_of_speeches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_part_of_speech\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mexercise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexercise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m# token_word = [[data[i].get_token(), data[i].get_user(), data[i].get_format(), data[i].get_part_of_speech()]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mappend\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(arr, values, axis)\u001b[0m\n\u001b[1;32m   4691\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4692\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4693\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "build_formatted_dataset(training_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(data):\n",
    "    \"\"\"\n",
    "    Mapping of each distinct feature to an unique index in a dictionary is used to create an embedding matrix for \n",
    "    each said distinct feature. Each individual embedding matrix will be concatenated\n",
    "    together to create one large embedding matrix.\n",
    "\n",
    "    Parameters:\n",
    "        data: a list of InstanceData objects from that data type and track.\n",
    "    Return:\n",
    "        feature_maxtrix_concat: concatenated embedding matrix\n",
    "    \"\"\"     \n",
    "    users, formats, tokens, part_of_speeches = feature_mapping(data)  \n",
    "\n",
    "    # Creating embedding matrices for each feature\n",
    "    print(\"Building embedding matrix...\")\n",
    "\n",
    "    # Embedding layers\n",
    "    user_tensor, format_tensor, token_tensor, pos_tensor = Input(shape = (None, ), name = \"users\"), Input(shape = (None, ), name = \"formats\"), Input(shape = (None, ), name = \"tokens\"), Input(shape = (None, ), name = \"pos\")\n",
    "    user_embed = layers.Embedding(len(users), 128) (user_tensor)\n",
    "    format_embed = layers.Embedding(len(formats), 16) (format_tensor)\n",
    "    token_embed = layers.Embedding(len(tokens), 512) (token_tensor)\n",
    "    pos_embed = layers.Embedding(len(part_of_speeches), 32) (pos_tensor)\n",
    "\n",
    "    embed_matrix = [user_embed, format_embed, token_embed, pos_embed]\n",
    "    embed_matrix_concat = layers.Concatenate()(feature_matrix)\n",
    "\n",
    "    print(\"Embedding matrix: {}\".format(embed_matrix_concat))\n",
    "\n",
    "    input_tensor = [user_tensor, format_tensor, token_tensor, pos_tensor]\n",
    "\n",
    "    return feature_matrix_concat, input_tensor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM(train_data, labels = None):\n",
    "   batch_size = 32\n",
    "   epochs = 10 \n",
    "\n",
    "   embedding, input_tensor = create_embeddings(train_data)\n",
    "   \n",
    "   # Generate the model\n",
    "   X = layers.LSTM(units = 256, return_sequences= True) (embedding)\n",
    "   X = layers.Dropout(0.25) (X)\n",
    "   X = layers.Dense(units = 128) (X)\n",
    "   X = layers.Activation('softmax') (X)\n",
    "\n",
    "   LSTM_model = Model(input_tensor, X)\n",
    "   LSTM_model.compile(optimizer = 'Adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "   LSTM_model.summary()\n",
    "\n",
    "\n",
    "   LSTM_hist = model.fit(\n",
    "      x = , \n",
    "      y = , \n",
    "      batch_size = batch_size,\n",
    "      epochs = epochs)\n",
    "\n",
    "   # return LSTM_hist\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Building embedding matrix...\nEmbedding matrix: Tensor(\"concatenate_4/Identity:0\", shape=(None, None, 656), dtype=float32)\nModel: \"model_4\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nusers (InputLayer)              [(None, None)]       0                                            \n__________________________________________________________________________________________________\nformats (InputLayer)            [(None, None)]       0                                            \n__________________________________________________________________________________________________\ntokens (InputLayer)             [(None, None)]       0                                            \n__________________________________________________________________________________________________\nembedding_12 (Embedding)        (None, None, 128)    331904      users[0][0]                      \n__________________________________________________________________________________________________\nembedding_13 (Embedding)        (None, None, 16)     48          formats[0][0]                    \n__________________________________________________________________________________________________\nembedding_14 (Embedding)        (None, None, 512)    1007104     tokens[0][0]                     \n__________________________________________________________________________________________________\nconcatenate_4 (Concatenate)     (None, None, 656)    0           embedding_12[0][0]               \n                                                                 embedding_13[0][0]               \n                                                                 embedding_14[0][0]               \n__________________________________________________________________________________________________\nlstm_4 (LSTM)                   (None, None, 256)    934912      concatenate_4[0][0]              \n__________________________________________________________________________________________________\ndropout_4 (Dropout)             (None, None, 256)    0           lstm_4[0][0]                     \n__________________________________________________________________________________________________\ndense_4 (Dense)                 (None, None, 128)    32896       dropout_4[0][0]                  \n__________________________________________________________________________________________________\nactivation_4 (Activation)       (None, None, 128)    0           dense_4[0][0]                    \n==================================================================================================\nTotal params: 2,306,864\nTrainable params: 2,306,864\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "LSTM(training_data, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}