{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit ('tensorflow': conda)",
   "metadata": {
    "interpreter": {
     "hash": "91633db7a21a9d87dbd5198446a164a72e34c28cbe9fa5b42d8cac37a5fb0989"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from eval import *\n",
    "from tensorflow.keras import preprocessing, utils, activations, optimizers\n",
    "from tensorflow.keras import Input, layers, models, metrics\n",
    "from sklearn.metrics import roc_curve,roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle('../data_fr_en/fr_en_train_data.pkl')\n",
    "test_data = pd.read_pickle('../data_fr_en/fr_en_test_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['label'] = train_data['label'].astype(int).astype(str)\n",
    "test_data['label'] = train_data['label'].astype(int).astype(str)\n",
    "train_data['country'] = train_data['country'].astype(str)\n",
    "test_data['country'] = test_data['country'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['user', 'country', 'format', 'session', 'token', 'part_of_speech', 'dependency_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_listings = pd.Series({feature : train_data[feature].tolist() for feature in features + ['label']})\n",
    "test_listings = pd.Series({feature : train_data[feature].tolist() for feature in features + ['label']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(user                [YjS/mQOx, YjS/mQOx, YjS/mQOx, YjS/mQOx, YjS/m...\n",
       " country             [['CA'], ['CA'], ['CA'], ['CA'], ['CA'], ['CA'...\n",
       " format              [reverse_translate, reverse_translate, reverse...\n",
       " session             [lesson, lesson, lesson, lesson, lesson, lesso...\n",
       " token               [le, garçon, je, suis, une, femme, la, fille, ...\n",
       " part_of_speech      [DET, NOUN, PRON, VERB, DET, NOUN, DET, NOUN, ...\n",
       " dependency_label    [det, ROOT, nsubj, cop, det, ROOT, det, ROOT, ...\n",
       " label               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       " dtype: object,\n",
       " user                [YjS/mQOx, YjS/mQOx, YjS/mQOx, YjS/mQOx, YjS/m...\n",
       " country             [['CA'], ['CA'], ['CA'], ['CA'], ['CA'], ['CA'...\n",
       " format              [reverse_translate, reverse_translate, reverse...\n",
       " session             [lesson, lesson, lesson, lesson, lesson, lesso...\n",
       " token               [le, garçon, je, suis, une, femme, la, fille, ...\n",
       " part_of_speech      [DET, NOUN, PRON, VERB, DET, NOUN, DET, NOUN, ...\n",
       " dependency_label    [det, ROOT, nsubj, cop, det, ROOT, det, ROOT, ...\n",
       " label               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       " dtype: object)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "train_listings, test_listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data_listified = train_data.groupby('user')[features + ['label']].apply(lambda data: pd.Series({feature : data[feature].tolist() for feature in features + ['label']}))\n",
    "test_data_listified = test_data.groupby('user')[features + ['label']].apply(lambda data: pd.Series({feature : data[feature].tolist() for feature in features + ['label']}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                       user  \\\n",
       "user                                                          \n",
       "++AP6DT7  [++AP6DT7, ++AP6DT7, ++AP6DT7, ++AP6DT7, ++AP6...   \n",
       "+0Dxf1dW  [+0Dxf1dW, +0Dxf1dW, +0Dxf1dW, +0Dxf1dW, +0Dxf...   \n",
       "+4kwmfjD  [+4kwmfjD, +4kwmfjD, +4kwmfjD, +4kwmfjD, +4kwm...   \n",
       "+5fX8ua2  [+5fX8ua2, +5fX8ua2, +5fX8ua2, +5fX8ua2, +5fX8...   \n",
       "+8m/uw4+  [+8m/uw4+, +8m/uw4+, +8m/uw4+, +8m/uw4+, +8m/u...   \n",
       "...                                                     ...   \n",
       "zdwKXRSt  [zdwKXRSt, zdwKXRSt, zdwKXRSt, zdwKXRSt, zdwKX...   \n",
       "zg2Sbl5B  [zg2Sbl5B, zg2Sbl5B, zg2Sbl5B, zg2Sbl5B, zg2Sb...   \n",
       "zgWENKZt  [zgWENKZt, zgWENKZt, zgWENKZt, zgWENKZt, zgWEN...   \n",
       "zmKqEwIw  [zmKqEwIw, zmKqEwIw, zmKqEwIw, zmKqEwIw, zmKqE...   \n",
       "zuKqgcjo  [zuKqgcjo, zuKqgcjo, zuKqgcjo, zuKqgcjo, zuKqg...   \n",
       "\n",
       "                                                    country  \\\n",
       "user                                                          \n",
       "++AP6DT7  [['TR'], ['TR'], ['TR'], ['TR'], ['TR'], ['TR'...   \n",
       "+0Dxf1dW  [['US'], ['US'], ['US'], ['US'], ['US'], ['US'...   \n",
       "+4kwmfjD  [['AU'], ['AU'], ['AU'], ['AU'], ['AU'], ['AU'...   \n",
       "+5fX8ua2  [['CA'], ['CA'], ['CA'], ['CA'], ['CA'], ['CA'...   \n",
       "+8m/uw4+  [['US'], ['US'], ['US'], ['US'], ['US'], ['US'...   \n",
       "...                                                     ...   \n",
       "zdwKXRSt  [['GB'], ['GB'], ['GB'], ['GB'], ['GB'], ['GB'...   \n",
       "zg2Sbl5B  [['EG'], ['EG'], ['EG'], ['EG'], ['EG'], ['EG'...   \n",
       "zgWENKZt  [['KE'], ['KE'], ['KE'], ['KE'], ['KE'], ['KE'...   \n",
       "zmKqEwIw  [['US'], ['US'], ['US'], ['US'], ['US'], ['US'...   \n",
       "zuKqgcjo  [['CN'], ['CN'], ['CN'], ['CN'], ['CN'], ['CN'...   \n",
       "\n",
       "                                                     format  \\\n",
       "user                                                          \n",
       "++AP6DT7  [reverse_tap, reverse_tap, reverse_tap, revers...   \n",
       "+0Dxf1dW  [reverse_translate, reverse_translate, reverse...   \n",
       "+4kwmfjD  [reverse_translate, reverse_translate, reverse...   \n",
       "+5fX8ua2  [reverse_tap, reverse_tap, reverse_tap, revers...   \n",
       "+8m/uw4+  [reverse_tap, reverse_tap, reverse_tap, revers...   \n",
       "...                                                     ...   \n",
       "zdwKXRSt  [reverse_tap, reverse_tap, reverse_tap, revers...   \n",
       "zg2Sbl5B  [reverse_tap, reverse_tap, reverse_tap, revers...   \n",
       "zgWENKZt  [reverse_tap, reverse_tap, reverse_tap, revers...   \n",
       "zmKqEwIw  [reverse_translate, reverse_translate, reverse...   \n",
       "zuKqgcjo  [reverse_tap, reverse_tap, reverse_tap, revers...   \n",
       "\n",
       "                                                    session  \\\n",
       "user                                                          \n",
       "++AP6DT7  [lesson, lesson, lesson, lesson, lesson, lesso...   \n",
       "+0Dxf1dW  [lesson, lesson, lesson, lesson, lesson, lesso...   \n",
       "+4kwmfjD  [lesson, lesson, lesson, lesson, lesson, lesso...   \n",
       "+5fX8ua2  [lesson, lesson, lesson, lesson, lesson, lesso...   \n",
       "+8m/uw4+  [lesson, lesson, lesson, lesson, lesson, lesso...   \n",
       "...                                                     ...   \n",
       "zdwKXRSt  [lesson, lesson, lesson, lesson, lesson, lesso...   \n",
       "zg2Sbl5B  [lesson, lesson, lesson, lesson, lesson, lesso...   \n",
       "zgWENKZt  [lesson, lesson, lesson, lesson, lesson, lesso...   \n",
       "zmKqEwIw  [lesson, lesson, lesson, lesson, lesson, lesso...   \n",
       "zuKqgcjo  [lesson, lesson, lesson, lesson, lesson, lesso...   \n",
       "\n",
       "                                                      token  \\\n",
       "user                                                          \n",
       "++AP6DT7  [je, suis, rouge, je, suis, riche, je, mange, ...   \n",
       "+0Dxf1dW  [la, femme, le, garçon, je, suis, rouge, je, s...   \n",
       "+4kwmfjD  [la, femme, je, suis, rouge, l', homme, je, su...   \n",
       "+5fX8ua2  [je, suis, riche, je, suis, rouge, l', homme, ...   \n",
       "+8m/uw4+  [je, suis, rouge, je, suis, riche, je, mange, ...   \n",
       "...                                                     ...   \n",
       "zdwKXRSt  [la, femme, je, suis, rouge, je, suis, riche, ...   \n",
       "zg2Sbl5B  [la, femme, le, garçon, une, pomme, je, suis, ...   \n",
       "zgWENKZt  [la, femme, je, suis, rouge, je, suis, riche, ...   \n",
       "zmKqEwIw  [une, pomme, l', homme, je, suis, rouge, je, s...   \n",
       "zuKqgcjo  [je, suis, rouge, une, pomme, je, suis, riche,...   \n",
       "\n",
       "                                             part_of_speech  \\\n",
       "user                                                          \n",
       "++AP6DT7  [PRON, VERB, ADJ, PRON, VERB, ADJ, PRON, VERB,...   \n",
       "+0Dxf1dW  [DET, NOUN, DET, NOUN, PRON, VERB, ADJ, PRON, ...   \n",
       "+4kwmfjD  [DET, NOUN, PRON, VERB, ADJ, DET, NOUN, PRON, ...   \n",
       "+5fX8ua2  [PRON, VERB, ADJ, PRON, VERB, ADJ, DET, NOUN, ...   \n",
       "+8m/uw4+  [PRON, VERB, ADJ, PRON, VERB, ADJ, PRON, VERB,...   \n",
       "...                                                     ...   \n",
       "zdwKXRSt  [DET, NOUN, PRON, VERB, ADJ, PRON, VERB, ADJ, ...   \n",
       "zg2Sbl5B  [DET, NOUN, DET, NOUN, DET, NOUN, PRON, VERB, ...   \n",
       "zgWENKZt  [DET, NOUN, PRON, VERB, ADJ, PRON, VERB, ADJ, ...   \n",
       "zmKqEwIw  [DET, NOUN, DET, NOUN, PRON, VERB, ADJ, PRON, ...   \n",
       "zuKqgcjo  [PRON, VERB, ADJ, DET, NOUN, PRON, VERB, ADJ, ...   \n",
       "\n",
       "                                           dependency_label  \\\n",
       "user                                                          \n",
       "++AP6DT7  [nsubj, cop, ROOT, nsubj, cop, ROOT, nsubj, RO...   \n",
       "+0Dxf1dW  [det, ROOT, det, ROOT, nsubj, cop, ROOT, nsubj...   \n",
       "+4kwmfjD  [det, ROOT, nsubj, cop, ROOT, det, ROOT, nsubj...   \n",
       "+5fX8ua2  [nsubj, cop, ROOT, nsubj, cop, ROOT, det, ROOT...   \n",
       "+8m/uw4+  [nsubj, cop, ROOT, nsubj, cop, ROOT, nsubj, RO...   \n",
       "...                                                     ...   \n",
       "zdwKXRSt  [det, ROOT, nsubj, cop, ROOT, nsubj, cop, ROOT...   \n",
       "zg2Sbl5B  [det, ROOT, det, ROOT, det, ROOT, nsubj, cop, ...   \n",
       "zgWENKZt  [det, ROOT, nsubj, cop, ROOT, nsubj, cop, ROOT...   \n",
       "zmKqEwIw  [det, ROOT, det, ROOT, nsubj, cop, ROOT, nsubj...   \n",
       "zuKqgcjo  [nsubj, cop, ROOT, det, ROOT, nsubj, cop, ROOT...   \n",
       "\n",
       "                                                      label  \n",
       "user                                                         \n",
       "++AP6DT7  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "+0Dxf1dW  [0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, ...  \n",
       "+4kwmfjD  [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "+5fX8ua2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "+8m/uw4+  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                     ...  \n",
       "zdwKXRSt  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...  \n",
       "zg2Sbl5B  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "zgWENKZt  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "zmKqEwIw  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "zuKqgcjo  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[1213 rows x 8 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user</th>\n      <th>country</th>\n      <th>format</th>\n      <th>session</th>\n      <th>token</th>\n      <th>part_of_speech</th>\n      <th>dependency_label</th>\n      <th>label</th>\n    </tr>\n    <tr>\n      <th>user</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>++AP6DT7</th>\n      <td>[++AP6DT7, ++AP6DT7, ++AP6DT7, ++AP6DT7, ++AP6...</td>\n      <td>[['TR'], ['TR'], ['TR'], ['TR'], ['TR'], ['TR'...</td>\n      <td>[reverse_tap, reverse_tap, reverse_tap, revers...</td>\n      <td>[lesson, lesson, lesson, lesson, lesson, lesso...</td>\n      <td>[je, suis, rouge, je, suis, riche, je, mange, ...</td>\n      <td>[PRON, VERB, ADJ, PRON, VERB, ADJ, PRON, VERB,...</td>\n      <td>[nsubj, cop, ROOT, nsubj, cop, ROOT, nsubj, RO...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>+0Dxf1dW</th>\n      <td>[+0Dxf1dW, +0Dxf1dW, +0Dxf1dW, +0Dxf1dW, +0Dxf...</td>\n      <td>[['US'], ['US'], ['US'], ['US'], ['US'], ['US'...</td>\n      <td>[reverse_translate, reverse_translate, reverse...</td>\n      <td>[lesson, lesson, lesson, lesson, lesson, lesso...</td>\n      <td>[la, femme, le, garçon, je, suis, rouge, je, s...</td>\n      <td>[DET, NOUN, DET, NOUN, PRON, VERB, ADJ, PRON, ...</td>\n      <td>[det, ROOT, det, ROOT, nsubj, cop, ROOT, nsubj...</td>\n      <td>[0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>+4kwmfjD</th>\n      <td>[+4kwmfjD, +4kwmfjD, +4kwmfjD, +4kwmfjD, +4kwm...</td>\n      <td>[['AU'], ['AU'], ['AU'], ['AU'], ['AU'], ['AU'...</td>\n      <td>[reverse_translate, reverse_translate, reverse...</td>\n      <td>[lesson, lesson, lesson, lesson, lesson, lesso...</td>\n      <td>[la, femme, je, suis, rouge, l', homme, je, su...</td>\n      <td>[DET, NOUN, PRON, VERB, ADJ, DET, NOUN, PRON, ...</td>\n      <td>[det, ROOT, nsubj, cop, ROOT, det, ROOT, nsubj...</td>\n      <td>[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>+5fX8ua2</th>\n      <td>[+5fX8ua2, +5fX8ua2, +5fX8ua2, +5fX8ua2, +5fX8...</td>\n      <td>[['CA'], ['CA'], ['CA'], ['CA'], ['CA'], ['CA'...</td>\n      <td>[reverse_tap, reverse_tap, reverse_tap, revers...</td>\n      <td>[lesson, lesson, lesson, lesson, lesson, lesso...</td>\n      <td>[je, suis, riche, je, suis, rouge, l', homme, ...</td>\n      <td>[PRON, VERB, ADJ, PRON, VERB, ADJ, DET, NOUN, ...</td>\n      <td>[nsubj, cop, ROOT, nsubj, cop, ROOT, det, ROOT...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>+8m/uw4+</th>\n      <td>[+8m/uw4+, +8m/uw4+, +8m/uw4+, +8m/uw4+, +8m/u...</td>\n      <td>[['US'], ['US'], ['US'], ['US'], ['US'], ['US'...</td>\n      <td>[reverse_tap, reverse_tap, reverse_tap, revers...</td>\n      <td>[lesson, lesson, lesson, lesson, lesson, lesso...</td>\n      <td>[je, suis, rouge, je, suis, riche, je, mange, ...</td>\n      <td>[PRON, VERB, ADJ, PRON, VERB, ADJ, PRON, VERB,...</td>\n      <td>[nsubj, cop, ROOT, nsubj, cop, ROOT, nsubj, RO...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>zdwKXRSt</th>\n      <td>[zdwKXRSt, zdwKXRSt, zdwKXRSt, zdwKXRSt, zdwKX...</td>\n      <td>[['GB'], ['GB'], ['GB'], ['GB'], ['GB'], ['GB'...</td>\n      <td>[reverse_tap, reverse_tap, reverse_tap, revers...</td>\n      <td>[lesson, lesson, lesson, lesson, lesson, lesso...</td>\n      <td>[la, femme, je, suis, rouge, je, suis, riche, ...</td>\n      <td>[DET, NOUN, PRON, VERB, ADJ, PRON, VERB, ADJ, ...</td>\n      <td>[det, ROOT, nsubj, cop, ROOT, nsubj, cop, ROOT...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>zg2Sbl5B</th>\n      <td>[zg2Sbl5B, zg2Sbl5B, zg2Sbl5B, zg2Sbl5B, zg2Sb...</td>\n      <td>[['EG'], ['EG'], ['EG'], ['EG'], ['EG'], ['EG'...</td>\n      <td>[reverse_tap, reverse_tap, reverse_tap, revers...</td>\n      <td>[lesson, lesson, lesson, lesson, lesson, lesso...</td>\n      <td>[la, femme, le, garçon, une, pomme, je, suis, ...</td>\n      <td>[DET, NOUN, DET, NOUN, DET, NOUN, PRON, VERB, ...</td>\n      <td>[det, ROOT, det, ROOT, det, ROOT, nsubj, cop, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>zgWENKZt</th>\n      <td>[zgWENKZt, zgWENKZt, zgWENKZt, zgWENKZt, zgWEN...</td>\n      <td>[['KE'], ['KE'], ['KE'], ['KE'], ['KE'], ['KE'...</td>\n      <td>[reverse_tap, reverse_tap, reverse_tap, revers...</td>\n      <td>[lesson, lesson, lesson, lesson, lesson, lesso...</td>\n      <td>[la, femme, je, suis, rouge, je, suis, riche, ...</td>\n      <td>[DET, NOUN, PRON, VERB, ADJ, PRON, VERB, ADJ, ...</td>\n      <td>[det, ROOT, nsubj, cop, ROOT, nsubj, cop, ROOT...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>zmKqEwIw</th>\n      <td>[zmKqEwIw, zmKqEwIw, zmKqEwIw, zmKqEwIw, zmKqE...</td>\n      <td>[['US'], ['US'], ['US'], ['US'], ['US'], ['US'...</td>\n      <td>[reverse_translate, reverse_translate, reverse...</td>\n      <td>[lesson, lesson, lesson, lesson, lesson, lesso...</td>\n      <td>[une, pomme, l', homme, je, suis, rouge, je, s...</td>\n      <td>[DET, NOUN, DET, NOUN, PRON, VERB, ADJ, PRON, ...</td>\n      <td>[det, ROOT, det, ROOT, nsubj, cop, ROOT, nsubj...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>zuKqgcjo</th>\n      <td>[zuKqgcjo, zuKqgcjo, zuKqgcjo, zuKqgcjo, zuKqg...</td>\n      <td>[['CN'], ['CN'], ['CN'], ['CN'], ['CN'], ['CN'...</td>\n      <td>[reverse_tap, reverse_tap, reverse_tap, revers...</td>\n      <td>[lesson, lesson, lesson, lesson, lesson, lesso...</td>\n      <td>[je, suis, rouge, une, pomme, je, suis, riche,...</td>\n      <td>[PRON, VERB, ADJ, DET, NOUN, PRON, VERB, ADJ, ...</td>\n      <td>[nsubj, cop, ROOT, det, ROOT, nsubj, cop, ROOT...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1213 rows × 8 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "train_data_listified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index to string mappings for the features, include an index mapping for padding\n",
    "# so padding will have index of 0\n",
    "def feature_mapping(data, pad = \"_PAD_\"):\n",
    "    feature_map = {}\n",
    "    for var in features + ['label']:\n",
    "        feature_map[var] = {}\n",
    "        unique_features = list(set(data[var]))\n",
    "        unique_features.insert(0, pad)\n",
    "\n",
    "        tokens_to_index = {feature: index for index, feature in enumerate(unique_features)}\n",
    "        feature_map[var] = tokens_to_index\n",
    "\n",
    "    return feature_map\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map_train = feature_mapping(train_data)\n",
    "feature_map_test = feature_mapping(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'user': 1214, 'country': 134, 'format': 4, 'session': 4, 'token': 1941, 'part_of_speech': 17, 'dependency_label': 33, 'label': 3}\n{'user': 1207, 'country': 134, 'format': 4, 'session': 4, 'token': 1707, 'part_of_speech': 17, 'dependency_label': 33, 'label': 3}\n"
     ]
    }
   ],
   "source": [
    "# Vocab sizes\n",
    "print({var: len(feature_map_train[var]) for var in feature_map_train})\n",
    "print({var: len(feature_map_test[var]) for var in feature_map_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(7675, 1055)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "max_length_train = train_data['user'].value_counts().max()\n",
    "max_length_test = test_data['user'].value_counts().max()\n",
    "max_length_train, max_length_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll just pad to the length of the longest individual sequence (apparently thats 8894)\n",
    "def add_padding(sequences, feature_map, maxlen = None):\n",
    "    index = [[feature_map[feature] for feature in sequence] for sequence in sequences]\n",
    "    index = preprocessing.sequence.pad_sequences(index, maxlen, value = feature_map[\"_PAD_\"])\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indexed_train_data = {var: add_padding(train_data_listified[var], feature_map_train[var], 2048) for var in feature_map_train}\n",
    "Y_train = indexed_train_data.pop('label')\n",
    "\n",
    "indexed_test_data = {var: add_padding(test_data_listified[var], feature_map_test[var]) for var in feature_map_test}\n",
    "Y_test = indexed_test_data.pop('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_oh = utils.to_categorical(Y_train)\n",
    "Y_test_oh = utils.to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((1213, 2048), (1206, 1055))"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "indexed_train_data['user'].shape, indexed_test_data['user'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((1213, 2048, 3), (1206, 1055, 3))"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "Y_train_oh.shape, Y_test_oh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(input_dim, output_dim, input_length = None, name = None):\n",
    "    input_tensor = Input(shape = (input_length, ), name = name)\n",
    "    embedding_layer = layers.Embedding(input_dim = input_dim, output_dim = output_dim, input_length = input_length, mask_zero = True, name = \"embedding_{}\".format(name))(input_tensor)\n",
    "\n",
    "    return input_tensor, embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embeddings(feature_map, max_length = None):\n",
    "    input_tensors = {}\n",
    "    output_tensors = {}\n",
    "\n",
    "    input_dims = {feature: len(feature_map[feature]) for feature in feature_map}\n",
    "    \n",
    "    output_dims = {}\n",
    "    for feature in features:\n",
    "        if len(feature_map[feature]) < 10:\n",
    "            output_dims[feature] = 8\n",
    "        if 10 <= len(feature_map[feature]) < 1000:\n",
    "            output_dims[feature] = 64\n",
    "        elif len(feature_map[feature]) >= 1000:\n",
    "            output_dims[feature] = 256\n",
    "  \n",
    "    for feature in features:\n",
    "        input_tensors[feature], output_tensors[feature] = create_embeddings(input_dims[feature], output_dims[feature], max_length, feature)\n",
    "\n",
    "    return input_tensors, output_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "({'user': <KerasTensor: shape=(None, None) dtype=float32 (created by layer 'user')>,\n",
       "  'country': <KerasTensor: shape=(None, None) dtype=float32 (created by layer 'country')>,\n",
       "  'format': <KerasTensor: shape=(None, None) dtype=float32 (created by layer 'format')>,\n",
       "  'session': <KerasTensor: shape=(None, None) dtype=float32 (created by layer 'session')>,\n",
       "  'token': <KerasTensor: shape=(None, None) dtype=float32 (created by layer 'token')>,\n",
       "  'part_of_speech': <KerasTensor: shape=(None, None) dtype=float32 (created by layer 'part_of_speech')>,\n",
       "  'dependency_label': <KerasTensor: shape=(None, None) dtype=float32 (created by layer 'dependency_label')>},\n",
       " {'user': <KerasTensor: shape=(None, None, 256) dtype=float32 (created by layer 'embedding_user')>,\n",
       "  'country': <KerasTensor: shape=(None, None, 64) dtype=float32 (created by layer 'embedding_country')>,\n",
       "  'format': <KerasTensor: shape=(None, None, 8) dtype=float32 (created by layer 'embedding_format')>,\n",
       "  'session': <KerasTensor: shape=(None, None, 8) dtype=float32 (created by layer 'embedding_session')>,\n",
       "  'token': <KerasTensor: shape=(None, None, 256) dtype=float32 (created by layer 'embedding_token')>,\n",
       "  'part_of_speech': <KerasTensor: shape=(None, None, 64) dtype=float32 (created by layer 'embedding_part_of_speech')>,\n",
       "  'dependency_label': <KerasTensor: shape=(None, None, 64) dtype=float32 (created by layer 'embedding_dependency_label')>})"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "input_tensors, output_tensors = build_embeddings(feature_map_train)\n",
    "input_tensors, output_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = [output_tensors[feature] for feature in features]\n",
    "embeddings_tensors = layers.Concatenate(name=\"embedding_all_features\")(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_layers(X, units, dropout = 0.0, num_layers = 1):\n",
    "    for j in range(num_layers):\n",
    "        X = layers.LSTM(units = units, dropout = dropout, return_sequences = True, name = \"LSTM_layer_{}\".format(num_layers))(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = lstm_layers(embeddings_tensors, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_layer(X, units, dropout, activation):\n",
    "    X = layers.Dropout(dropout)(X)\n",
    "    X = layers.Dense(units = units, activation = activation)(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dense_layer(X, 256, 0.2, 'softmax')\n",
    "outputs = dense_layer(X, 3, 0.1, 'sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nuser (InputLayer)               [(None, None)]       0                                            \n__________________________________________________________________________________________________\ncountry (InputLayer)            [(None, None)]       0                                            \n__________________________________________________________________________________________________\nformat (InputLayer)             [(None, None)]       0                                            \n__________________________________________________________________________________________________\nsession (InputLayer)            [(None, None)]       0                                            \n__________________________________________________________________________________________________\ntoken (InputLayer)              [(None, None)]       0                                            \n__________________________________________________________________________________________________\npart_of_speech (InputLayer)     [(None, None)]       0                                            \n__________________________________________________________________________________________________\ndependency_label (InputLayer)   [(None, None)]       0                                            \n__________________________________________________________________________________________________\nembedding_user (Embedding)      (None, None, 256)    310784      user[0][0]                       \n__________________________________________________________________________________________________\nembedding_country (Embedding)   (None, None, 64)     8576        country[0][0]                    \n__________________________________________________________________________________________________\nembedding_format (Embedding)    (None, None, 8)      32          format[0][0]                     \n__________________________________________________________________________________________________\nembedding_session (Embedding)   (None, None, 8)      32          session[0][0]                    \n__________________________________________________________________________________________________\nembedding_token (Embedding)     (None, None, 256)    496896      token[0][0]                      \n__________________________________________________________________________________________________\nembedding_part_of_speech (Embed (None, None, 64)     1088        part_of_speech[0][0]             \n__________________________________________________________________________________________________\nembedding_dependency_label (Emb (None, None, 64)     2112        dependency_label[0][0]           \n__________________________________________________________________________________________________\nembedding_all_features (Concate (None, None, 720)    0           embedding_user[0][0]             \n                                                                 embedding_country[0][0]          \n                                                                 embedding_format[0][0]           \n                                                                 embedding_session[0][0]          \n                                                                 embedding_token[0][0]            \n                                                                 embedding_part_of_speech[0][0]   \n                                                                 embedding_dependency_label[0][0] \n__________________________________________________________________________________________________\nLSTM_layer_1 (LSTM)             (None, None, 256)    1000448     embedding_all_features[0][0]     \n__________________________________________________________________________________________________\ndropout (Dropout)               (None, None, 256)    0           LSTM_layer_1[0][0]               \n__________________________________________________________________________________________________\ndense (Dense)                   (None, None, 256)    65792       dropout[0][0]                    \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, None, 256)    0           dense[0][0]                      \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, None, 3)      771         dropout_1[0][0]                  \n==================================================================================================\nTotal params: 1,886,531\nTrainable params: 1,886,531\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = [input_tensors[feature] for feature in features]\n",
    "LSTM_model = models.Model(inputs = inputs, outputs = outputs)\n",
    "LSTM_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [indexed_train_data[feature] for feature in features]\n",
    "X_test = [indexed_test_data[feature] for feature in features]\n",
    "\n",
    "optimizer = optimizers.Adam(learning_rate = 0.001)\n",
    "LSTM_model.compile(optimizer = optimizer, loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "19/19 [==============================] - 52s 3s/step - loss: 0.2432 - accuracy: 0.3110 - auc: 0.5456 - precision: 0.3123 - recall: 0.3083 - val_loss: 0.0706 - val_accuracy: 0.8377 - val_auc: 0.9136 - val_precision: 0.8377 - val_recall: 0.8377\n",
      "Epoch 2/20\n",
      "19/19 [==============================] - 46s 2s/step - loss: 0.2276 - accuracy: 0.8364 - auc: 0.9128 - precision: 0.8364 - recall: 0.8364 - val_loss: 0.0668 - val_accuracy: 0.8377 - val_auc: 0.9159 - val_precision: 0.8377 - val_recall: 0.8377\n",
      "Epoch 3/20\n",
      "19/19 [==============================] - 46s 2s/step - loss: 0.2219 - accuracy: 0.8388 - auc: 0.9110 - precision: 0.8388 - recall: 0.8388 - val_loss: 0.0648 - val_accuracy: 0.8377 - val_auc: 0.9170 - val_precision: 0.8377 - val_recall: 0.8377\n",
      "Epoch 4/20\n",
      "19/19 [==============================] - 47s 2s/step - loss: 0.2097 - accuracy: 0.8378 - auc: 0.9110 - precision: 0.8378 - recall: 0.8378 - val_loss: 0.0631 - val_accuracy: 0.8377 - val_auc: 0.9167 - val_precision: 0.8377 - val_recall: 0.8377\n",
      "Epoch 5/20\n",
      "19/19 [==============================] - 45s 2s/step - loss: 0.2051 - accuracy: 0.8381 - auc: 0.8909 - precision: 0.8381 - recall: 0.8381 - val_loss: 0.0612 - val_accuracy: 0.8377 - val_auc: 0.8784 - val_precision: 0.8377 - val_recall: 0.8377\n",
      "Epoch 6/20\n",
      "19/19 [==============================] - 47s 2s/step - loss: 0.1987 - accuracy: 0.8352 - auc: 0.8718 - precision: 0.8352 - recall: 0.8352 - val_loss: 0.0595 - val_accuracy: 0.8377 - val_auc: 0.8784 - val_precision: 0.8377 - val_recall: 0.8377\n",
      "Epoch 7/20\n",
      "19/19 [==============================] - 45s 2s/step - loss: 0.1945 - accuracy: 0.8356 - auc: 0.8713 - precision: 0.8356 - recall: 0.8356 - val_loss: 0.0581 - val_accuracy: 0.8377 - val_auc: 0.8784 - val_precision: 0.8377 - val_recall: 0.8377\n",
      "Epoch 8/20\n",
      "19/19 [==============================] - 45s 2s/step - loss: 0.1883 - accuracy: 0.8377 - auc: 0.8790 - precision: 0.8377 - recall: 0.8377 - val_loss: 0.0568 - val_accuracy: 0.8377 - val_auc: 0.8784 - val_precision: 0.8377 - val_recall: 0.8377\n",
      "Epoch 9/20\n",
      "19/19 [==============================] - 45s 2s/step - loss: 0.1875 - accuracy: 0.8402 - auc: 0.8835 - precision: 0.8402 - recall: 0.8402 - val_loss: 0.0556 - val_accuracy: 0.8377 - val_auc: 0.8785 - val_precision: 0.8377 - val_recall: 0.8377\n",
      "Epoch 10/20\n",
      "19/19 [==============================] - 45s 2s/step - loss: 0.1855 - accuracy: 0.8376 - auc: 0.8815 - precision: 0.8376 - recall: 0.8376 - val_loss: 0.0546 - val_accuracy: 0.8377 - val_auc: 0.9173 - val_precision: 0.8377 - val_recall: 0.8377\n",
      "Epoch 11/20\n",
      "19/19 [==============================] - 45s 2s/step - loss: 0.1853 - accuracy: 0.8387 - auc: 0.8899 - precision: 0.8387 - recall: 0.8387 - val_loss: 0.0536 - val_accuracy: 0.8377 - val_auc: 0.8784 - val_precision: 0.8377 - val_recall: 0.8377\n",
      "Epoch 12/20\n",
      "19/19 [==============================] - 45s 2s/step - loss: 0.1767 - accuracy: 0.8401 - auc: 0.9012 - precision: 0.8401 - recall: 0.8401 - val_loss: 0.0526 - val_accuracy: 0.8377 - val_auc: 0.9175 - val_precision: 0.8377 - val_recall: 0.8377\n",
      "Epoch 13/20\n",
      "19/19 [==============================] - 45s 2s/step - loss: 0.1791 - accuracy: 0.8376 - auc: 0.8960 - precision: 0.8376 - recall: 0.8376 - val_loss: 0.0518 - val_accuracy: 0.8377 - val_auc: 0.9175 - val_precision: 0.8377 - val_recall: 0.8377\n",
      "Epoch 14/20\n",
      "19/19 [==============================] - 46s 2s/step - loss: 0.1742 - accuracy: 0.8375 - auc: 0.9046 - precision: 0.8375 - recall: 0.8375 - val_loss: 0.0509 - val_accuracy: 0.8377 - val_auc: 0.9177 - val_precision: 0.8377 - val_recall: 0.8377\n",
      "Epoch 15/20\n",
      "19/19 [==============================] - 45s 2s/step - loss: 0.1694 - accuracy: 0.8351 - auc: 0.9082 - precision: 0.8351 - recall: 0.8351 - val_loss: 0.0501 - val_accuracy: 0.8377 - val_auc: 0.9177 - val_precision: 0.8377 - val_recall: 0.8377\n",
      "Epoch 16/20\n",
      "19/19 [==============================] - 45s 2s/step - loss: 0.1648 - accuracy: 0.8396 - auc: 0.9097 - precision: 0.8396 - recall: 0.8396 - val_loss: 0.0494 - val_accuracy: 0.8377 - val_auc: 0.9179 - val_precision: 0.8377 - val_recall: 0.8377\n",
      "Epoch 17/20\n",
      "19/19 [==============================] - 44s 2s/step - loss: 0.1639 - accuracy: 0.8382 - auc: 0.9115 - precision: 0.8382 - recall: 0.8382 - val_loss: 0.0487 - val_accuracy: 0.8377 - val_auc: 0.9180 - val_precision: 0.8377 - val_recall: 0.8377\n",
      "Epoch 18/20\n",
      "19/19 [==============================] - 44s 2s/step - loss: 0.1626 - accuracy: 0.8385 - auc: 0.9113 - precision: 0.8385 - recall: 0.8385 - val_loss: 0.0480 - val_accuracy: 0.8377 - val_auc: 0.9180 - val_precision: 0.8377 - val_recall: 0.8377\n",
      "Epoch 19/20\n",
      "19/19 [==============================] - 45s 2s/step - loss: 0.1587 - accuracy: 0.8381 - auc: 0.9117 - precision: 0.8381 - recall: 0.8381 - val_loss: 0.0474 - val_accuracy: 0.8377 - val_auc: 0.9181 - val_precision: 0.8377 - val_recall: 0.8377\n",
      "Epoch 20/20\n",
      "19/19 [==============================] - 47s 2s/step - loss: 0.1594 - accuracy: 0.8323 - auc: 0.9089 - precision: 0.8323 - recall: 0.8323 - val_loss: 0.0468 - val_accuracy: 0.8377 - val_auc: 0.9180 - val_precision: 0.8377 - val_recall: 0.8377\n"
     ]
    }
   ],
   "source": [
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "LSTM_history = LSTM_model.fit(\n",
    "    x = X_train,\n",
    "    y = Y_train_oh,\n",
    "    batch_size = 64,\n",
    "    epochs = 20,\n",
    "    validation_data = (X_test, Y_test_oh),\n",
    "    shuffle = True,\n",
    "    callbacks = [callbacks]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "INFO:tensorflow:Assets written to: LSTM_model_fr_en/assets\n",
      "INFO:tensorflow:Assets written to: LSTM_model_fr_en/assets\n"
     ]
    }
   ],
   "source": [
    "LSTM_model.save('LSTM_model_fr_en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM_model = models.load_model('LSTM_model_en_es/')\n",
    "Y_test_pred = LSTM_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(Y, Y_pred):\n",
    "    Y_condensed = Y.reshape(-1, 3)\n",
    "    Y_pred_condensed = Y_pred.reshape(-1, 3)\n",
    "    \n",
    "   \n",
    "    # If first vertical axis is a 0, then there's padding\n",
    "    padded = np.where(Y_condensed[:, 0] == 0)[0]\n",
    "    Y_condensed = Y_condensed[padded, :]\n",
    "    Y_pred_condensed = Y_pred_condensed[padded, :]\n",
    "\n",
    "    test_metrics()\n",
    "    metrics = evaluate_metrics(Y_condensed[:, -1], Y_pred_condensed[:, -1])\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Verified that our environment is calculating metrics correctly.\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate_model(Y_test_oh, Y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'accuracy': 0.08922763748398607,\n",
       " 'avglogloss': 0.8359590496177308,\n",
       " 'auroc': 0.9905292502086208,\n",
       " 'F1': 0.16383652858844744}"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"fr_en_test_results.txt\", 'w')\n",
    "f.write(str(metrics))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}